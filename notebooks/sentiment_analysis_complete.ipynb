{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dee7d222",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Social Media Text\n",
    "\n",
    "**Author:** Sadegh Rad  \n",
    "**Project:** DSL2122 January Dataset Sentiment Classification  \n",
    "**Date:** 2024\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates a comprehensive sentiment analysis pipeline for classifying social media text (tweets) into positive and negative sentiments. The project implements:\n",
    "\n",
    "- **Data Preprocessing**: Text cleaning, normalization, and balancing\n",
    "- **Feature Engineering**: TF-IDF vectorization\n",
    "- **Model Training**: Support Vector Machine, Logistic Regression, and Naive Bayes\n",
    "- **Evaluation**: Performance metrics and visualization\n",
    "\n",
    "## Dataset\n",
    "\n",
    "- **Training Data**: 224,996 labeled tweets (development.csv)\n",
    "- **Test Data**: 75,001 unlabeled tweets (evaluation.csv)\n",
    "- **Labels**: Binary classification (0: negative, 1: positive)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db8cd2",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9796d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Environment setup completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2bcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df_dev = pd.read_csv('../data/raw/development.csv', sep=',', encoding=\"ISO-8859-1\", index_col=False)\n",
    "df_eval = pd.read_csv('../data/raw/evaluation.csv', sep=',', encoding=\"ISO-8859-1\", index_col=False)\n",
    "\n",
    "print(f\"Development dataset shape: {df_dev.shape}\")\n",
    "print(f\"Evaluation dataset shape: {df_eval.shape}\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nDevelopment dataset columns:\", df_dev.columns.tolist())\n",
    "print(\"Evaluation dataset columns:\", df_eval.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142922f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine sample data\n",
    "print(\"Sample of development data:\")\n",
    "df_dev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1750d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "class_distribution = df_dev['sentiment'].value_counts()\n",
    "print(\"Class Distribution:\")\n",
    "print(f\"Positive (1): {class_distribution[1]:,} samples ({class_distribution[1]/len(df_dev)*100:.1f}%)\")\n",
    "print(f\"Negative (0): {class_distribution[0]:,} samples ({class_distribution[0]/len(df_dev)*100:.1f}%)\")\n",
    "\n",
    "# Visualize class distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "class_distribution.plot(kind='bar', color=['lightcoral', 'lightblue'])\n",
    "plt.title('Class Distribution in Training Data')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Negative (0)', 'Positive (1)'], rotation=0)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754995ab",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing\n",
    "\n",
    "The preprocessing pipeline includes:\n",
    "1. Data balancing (downsampling majority class)\n",
    "2. Text cleaning and normalization\n",
    "3. Tokenization and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baac7109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing utilities\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Balance the dataset by downsampling the majority class\n",
    "df_majority = df_dev[df_dev.sentiment == 1]\n",
    "df_minority = df_dev[df_dev.sentiment == 0]\n",
    "\n",
    "print(f\"Original sizes - Majority: {len(df_majority)}, Minority: {len(df_minority)}\")\n",
    "\n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                   replace=False,    \n",
    "                                   n_samples=len(df_minority),\n",
    "                                   random_state=10)\n",
    "\n",
    "# Combine minority class with downsampled majority class\n",
    "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
    "\n",
    "print(f\"\\nBalanced dataset:\")\n",
    "print(df_balanced.sentiment.value_counts())\n",
    "print(f\"Total samples: {len(df_balanced)}\")\n",
    "\n",
    "# Update development dataset\n",
    "df_dev = df_balanced.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb654e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing functions\n",
    "import re\n",
    "import string\n",
    "\n",
    "def clean_username(text):\n",
    "    \"\"\"Remove @mentions from text\"\"\"\n",
    "    return re.sub('@[^\\s]+', '', text)\n",
    "\n",
    "def clean_urls(text):\n",
    "    \"\"\"Remove URLs from text\"\"\"\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "def clean_html(text):\n",
    "    \"\"\"Remove HTML tags from text\"\"\"\n",
    "    html_pattern = re.compile(r'<[^>]*>')\n",
    "    return html_pattern.sub('', text)\n",
    "\n",
    "def expand_contractions(text):\n",
    "    \"\"\"Expand contractions in text\"\"\"\n",
    "    contractions_dict = {\n",
    "        r\"won\\'t\": \" will not\",\n",
    "        r\"can\\'t\": \" can not\",\n",
    "        r\"don\\'t\": \" do not\",\n",
    "        r\"ain\\'t\": \" am not\",\n",
    "        r\"n\\'t\": \" not\",\n",
    "        r\"\\'re\": \" are\",\n",
    "        r\"\\'s\": \" is\",\n",
    "        r\"\\'d\": \" would\",\n",
    "        r\"\\'ll\": \" will\",\n",
    "        r\"\\'ve\": \" have\",\n",
    "        r\"\\'m\": \" am\"\n",
    "    }\n",
    "    \n",
    "    for contraction, expansion in contractions_dict.items():\n",
    "        text = re.sub(contraction, expansion, text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_numbers(text):\n",
    "    \"\"\"Remove numeric characters\"\"\"\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def keep_alpha_only(text):\n",
    "    \"\"\"Keep only alphabetic characters\"\"\"\n",
    "    return re.sub(r'[^a-zA-Z]', ' ', text)\n",
    "\n",
    "def remove_short_words(text, min_length=3):\n",
    "    \"\"\"Remove words shorter than min_length\"\"\"\n",
    "    words = text.split()\n",
    "    return ' '.join([word for word in words if len(word) >= min_length])\n",
    "\n",
    "print(\"Text cleaning functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874c686e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define comprehensive stopwords list\n",
    "STOPWORDS = {\n",
    "    'a', 'about', 'above', 'after', 'again', 'ain', 'all', 'am', 'an',\n",
    "    'and', 'any', 'are', 'as', 'at', 'be', 'because', 'been', 'before',\n",
    "    'being', 'below', 'between', 'both', 'by', 'can', 'd', 'did', 'do',\n",
    "    'does', 'doing', 'down', 'during', 'each', 'few', 'for', 'from',\n",
    "    'further', 'had', 'has', 'have', 'having', 'he', 'her', 'here',\n",
    "    'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in',\n",
    "    'into', 'is', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma',\n",
    "    'me', 'more', 'most', 'my', 'myself', 'now', 'o', 'of', 'on', 'once',\n",
    "    'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'own', 're', \n",
    "    's', 'same', 'she', 'shes', 'should', 'shouldve', 'so', 'some', 'such',\n",
    "    't', 'than', 'that', 'thatll', 'the', 'their', 'theirs', 'them',\n",
    "    'themselves', 'then', 'there', 'these', 'they', 'this', 'those',\n",
    "    'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was',\n",
    "    'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom',\n",
    "    'why', 'will', 'with', 'won', 'y', 'you', 'youd', 'youll', 'youre',\n",
    "    'youve', 'your', 'yours', 'yourself', 'yourselves', 'aww', 'loud', \n",
    "    'get', 'quot', 'amp', 'would', 'could', 'yes', 'though', 'but', \n",
    "    'haha', 'hahaha', 'dont', 'cant', 'even', 'tho', 'already', 'yet', \n",
    "    'hehe', 'lot', 'love', 'think', 'know', 'one', 'go', 'today', 'see', \n",
    "    'time', 'work', 'make', 'say', 'yeah', 'way', 'laugh'\n",
    "}\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"Remove stopwords from text\"\"\"\n",
    "    words = text.split()\n",
    "    return \" \".join([word for word in words if word.lower() not in STOPWORDS])\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"Remove punctuation from text\"\"\"\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return str(text).translate(translator)\n",
    "\n",
    "print(f\"Stopwords defined: {len(STOPWORDS)} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f45a555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing pipeline\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Apply complete preprocessing pipeline\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to string and lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Apply cleaning functions\n",
    "    text = clean_username(text)\n",
    "    text = clean_urls(text)\n",
    "    text = clean_html(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = clean_numbers(text)\n",
    "    text = keep_alpha_only(text)\n",
    "    text = remove_short_words(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_punctuation(text)\n",
    "    \n",
    "    # Clean extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Process development data\n",
    "print(\"Processing development data...\")\n",
    "df_dev['text_cleaned'] = df_dev['text'].apply(preprocess_text)\n",
    "\n",
    "# Process evaluation data\n",
    "print(\"Processing evaluation data...\")\n",
    "df_eval['text_cleaned'] = df_eval['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Text preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd705c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples of preprocessing\n",
    "print(\"Examples of text preprocessing:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Original: {df_dev.iloc[i]['text'][:100]}...\")\n",
    "    print(f\"Cleaned:  {df_dev.iloc[i]['text_cleaned'][:100]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5605fd85",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis\n",
    "\n",
    "Let's analyze the preprocessed text data to understand patterns and word distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e587262e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze text lengths\n",
    "df_dev['text_length'] = df_dev['text_cleaned'].str.len()\n",
    "df_dev['word_count'] = df_dev['text_cleaned'].str.split().str.len()\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Text length distribution\n",
    "axes[0, 0].hist(df_dev['text_length'], bins=50, alpha=0.7, color='skyblue')\n",
    "axes[0, 0].set_title('Distribution of Text Lengths (Characters)')\n",
    "axes[0, 0].set_xlabel('Text Length')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Word count distribution\n",
    "axes[0, 1].hist(df_dev['word_count'], bins=50, alpha=0.7, color='lightgreen')\n",
    "axes[0, 1].set_title('Distribution of Word Counts')\n",
    "axes[0, 1].set_xlabel('Word Count')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Text length by sentiment\n",
    "for sentiment in [0, 1]:\n",
    "    sentiment_data = df_dev[df_dev['sentiment'] == sentiment]['text_length']\n",
    "    axes[1, 0].hist(sentiment_data, bins=30, alpha=0.6, \n",
    "                   label=f'Sentiment {sentiment}', density=True)\n",
    "axes[1, 0].set_title('Text Length Distribution by Sentiment')\n",
    "axes[1, 0].set_xlabel('Text Length')\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Word count by sentiment\n",
    "for sentiment in [0, 1]:\n",
    "    sentiment_data = df_dev[df_dev['sentiment'] == sentiment]['word_count']\n",
    "    axes[1, 1].hist(sentiment_data, bins=30, alpha=0.6, \n",
    "                   label=f'Sentiment {sentiment}', density=True)\n",
    "axes[1, 1].set_title('Word Count Distribution by Sentiment')\n",
    "axes[1, 1].set_xlabel('Word Count')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(\"Text Length Statistics:\")\n",
    "print(df_dev['text_length'].describe())\n",
    "print(\"\\nWord Count Statistics:\")\n",
    "print(df_dev['word_count'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeea994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word frequency analysis\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create word frequency analysis\n",
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "word_counts = vectorizer.fit_transform(df_dev['text_cleaned'].astype(str))\n",
    "\n",
    "# Get word frequencies\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "word_freq = word_counts.sum(axis=0).A1\n",
    "word_freq_df = pd.DataFrame({\n",
    "    'word': feature_names,\n",
    "    'frequency': word_freq\n",
    "}).sort_values('frequency', ascending=False)\n",
    "\n",
    "# Plot top 20 words\n",
    "plt.figure(figsize=(12, 8))\n",
    "top_words = word_freq_df.head(20)\n",
    "plt.barh(range(len(top_words)), top_words['frequency'], color='steelblue')\n",
    "plt.yticks(range(len(top_words)), top_words['word'])\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 20 Most Frequent Words')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Top 10 most frequent words:\")\n",
    "print(word_freq_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a3145a",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "We'll use TF-IDF (Term Frequency-Inverse Document Frequency) to convert text into numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4f292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Extract features and target\n",
    "X = df_dev['text_cleaned'].astype(str)\n",
    "y = df_dev['sentiment'].astype(str)\n",
    "X_eval = df_eval['text_cleaned'].astype(str)\n",
    "\n",
    "# Split training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Validation set: {len(X_test)} samples\")\n",
    "print(f\"Evaluation set: {len(X_eval)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    min_df=5,        # Ignore terms that appear in less than 5 documents\n",
    "    max_df=0.8,      # Ignore terms that appear in more than 80% of documents\n",
    "    sublinear_tf=True,  # Use sublinear tf scaling\n",
    "    use_idf=True     # Use inverse document frequency\n",
    ")\n",
    "\n",
    "# Fit on training data and transform\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "X_eval_tfidf = tfidf_vectorizer.transform(X_eval)\n",
    "\n",
    "print(f\"Number of TF-IDF features: {X_train_tfidf.shape[1]}\")\n",
    "print(f\"Training matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Validation matrix shape: {X_test_tfidf.shape}\")\n",
    "print(f\"Evaluation matrix shape: {X_eval_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30600c12",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation\n",
    "\n",
    "We'll train three different machine learning models and compare their performance:\n",
    "1. **Support Vector Machine (Linear SVM)**\n",
    "2. **Logistic Regression**\n",
    "3. **Naive Bayes (Bernoulli)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e04668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models and evaluation metrics\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Linear SVM': LinearSVC(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(\n",
    "        solver='saga', \n",
    "        fit_intercept=True, \n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    ),\n",
    "    'Naive Bayes': BernoulliNB()\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'predictions': y_pred,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nModel training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce16dd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "model_names = list(model_results.keys())\n",
    "accuracies = [model_results[name]['accuracy'] for name in model_names]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, accuracies, color=['lightblue', 'lightgreen', 'lightcoral'])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.7, 0.9)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.005, \n",
    "             f'{acc:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find best model\n",
    "best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['accuracy'])\n",
    "best_accuracy = model_results[best_model_name]['accuracy']\n",
    "\n",
    "print(f\"\\nBest Model: {best_model_name}\")\n",
    "print(f\"Best Accuracy: {best_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef5541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for idx, (name, results) in enumerate(model_results.items()):\n",
    "    cm = confusion_matrix(y_test, results['predictions'])\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                ax=axes[idx],\n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    \n",
    "    axes[idx].set_title(f'{name}\\nAccuracy: {results[\"accuracy\"]:.4f}')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    if idx == 0:\n",
    "        axes[idx].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3554ea3d",
   "metadata": {},
   "source": [
    "## 6. Final Predictions\n",
    "\n",
    "Using the best performing model to make predictions on the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f362c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make final predictions using the best model\n",
    "best_model = model_results[best_model_name]['model']\n",
    "final_predictions = best_model.predict(X_eval_tfidf)\n",
    "\n",
    "print(f\"Making final predictions using {best_model_name}...\")\n",
    "print(f\"Number of predictions: {len(final_predictions)}\")\n",
    "\n",
    "# Analyze prediction distribution\n",
    "pred_distribution = pd.Series(final_predictions).value_counts()\n",
    "print(f\"\\nPrediction Distribution:\")\n",
    "print(f\"Negative (0): {pred_distribution.get('0', 0)} ({pred_distribution.get('0', 0)/len(final_predictions)*100:.1f}%)\")\n",
    "print(f\"Positive (1): {pred_distribution.get('1', 0)} ({pred_distribution.get('1', 0)/len(final_predictions)*100:.1f}%)\")\n",
    "\n",
    "# Visualize prediction distribution\n",
    "plt.figure(figsize=(8, 6))\n",
    "pred_distribution.plot(kind='bar', color=['lightcoral', 'lightblue'])\n",
    "plt.title('Distribution of Final Predictions')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Negative (0)', 'Positive (1)'], rotation=0)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a5551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV file\n",
    "predictions_df = pd.DataFrame(final_predictions, columns=['Predicted'])\n",
    "predictions_df.index.name = 'Id'\n",
    "\n",
    "# Save to results directory\n",
    "import os\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "predictions_df.to_csv('../results/final_predictions.csv')\n",
    "\n",
    "print(\"Predictions saved to '../results/final_predictions.csv'\")\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(predictions_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42394a5",
   "metadata": {},
   "source": [
    "## 7. Model Analysis and Feature Importance\n",
    "\n",
    "Let's analyze what features (words) are most important for our best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91948c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from the best model\n",
    "if hasattr(best_model, 'coef_'):\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    coefficients = best_model.coef_[0]\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'coefficient': coefficients,\n",
    "        'abs_coefficient': np.abs(coefficients)\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    # Get top positive and negative features\n",
    "    top_positive = feature_importance.nlargest(10, 'coefficient')\n",
    "    top_negative = feature_importance.nsmallest(10, 'coefficient')\n",
    "    \n",
    "    # Visualize feature importance\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Top positive features\n",
    "    axes[0].barh(range(len(top_positive)), top_positive['coefficient'], color='lightgreen')\n",
    "    axes[0].set_yticks(range(len(top_positive)))\n",
    "    axes[0].set_yticklabels(top_positive['feature'])\n",
    "    axes[0].set_title('Top 10 Positive Sentiment Features')\n",
    "    axes[0].set_xlabel('Coefficient Value')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Top negative features\n",
    "    axes[1].barh(range(len(top_negative)), top_negative['coefficient'], color='lightcoral')\n",
    "    axes[1].set_yticks(range(len(top_negative)))\n",
    "    axes[1].set_yticklabels(top_negative['feature'])\n",
    "    axes[1].set_title('Top 10 Negative Sentiment Features')\n",
    "    axes[1].set_xlabel('Coefficient Value')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Top 10 words associated with POSITIVE sentiment:\")\n",
    "    print(top_positive[['feature', 'coefficient']].to_string(index=False))\n",
    "    \n",
    "    print(\"\\nTop 10 words associated with NEGATIVE sentiment:\")\n",
    "    print(top_negative[['feature', 'coefficient']].to_string(index=False))\n",
    "    \n",
    "else:\n",
    "    print(f\"Feature importance analysis not available for {best_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba3b964",
   "metadata": {},
   "source": [
    "## 8. Summary and Conclusions\n",
    "\n",
    "This notebook demonstrated a complete sentiment analysis pipeline for social media text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07972bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final summary\n",
    "print(\"=\" * 60)\n",
    "print(\"SENTIMENT ANALYSIS PROJECT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä Dataset Statistics:\")\n",
    "print(f\"   ‚Ä¢ Training samples: {len(df_dev):,}\")\n",
    "print(f\"   ‚Ä¢ Test samples: {len(df_eval):,}\")\n",
    "print(f\"   ‚Ä¢ Features (TF-IDF): {X_train_tfidf.shape[1]:,}\")\n",
    "\n",
    "print(f\"\\nü§ñ Models Evaluated:\")\n",
    "for name, results in model_results.items():\n",
    "    print(f\"   ‚Ä¢ {name}: {results['accuracy']:.4f} accuracy\")\n",
    "\n",
    "print(f\"\\nüèÜ Best Model:\")\n",
    "print(f\"   ‚Ä¢ Model: {best_model_name}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"   ‚Ä¢ Predictions saved to: ../results/final_predictions.csv\")\n",
    "\n",
    "print(f\"\\nüìà Key Achievements:\")\n",
    "print(f\"   ‚Ä¢ Comprehensive text preprocessing pipeline\")\n",
    "print(f\"   ‚Ä¢ Balanced dataset through downsampling\")\n",
    "print(f\"   ‚Ä¢ TF-IDF feature extraction with {X_train_tfidf.shape[1]:,} features\")\n",
    "print(f\"   ‚Ä¢ Evaluation of 3 different ML algorithms\")\n",
    "print(f\"   ‚Ä¢ {best_accuracy:.1%} accuracy on validation set\")\n",
    "\n",
    "print(f\"\\nüí° Technical Highlights:\")\n",
    "print(f\"   ‚Ä¢ Data balancing to handle class imbalance\")\n",
    "print(f\"   ‚Ä¢ Advanced text preprocessing (contractions, stopwords, lemmatization)\")\n",
    "print(f\"   ‚Ä¢ TF-IDF vectorization with optimal parameters\")\n",
    "print(f\"   ‚Ä¢ Model comparison and evaluation\")\n",
    "print(f\"   ‚Ä¢ Feature importance analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PROJECT COMPLETED SUCCESSFULLY! üéâ\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
